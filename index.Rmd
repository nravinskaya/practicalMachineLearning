---
title: "Predicting the correctness of exercise performance"
author: "Natalia Ravinskaya"
date: "July 7, 2020"
output: 
  html_document:
    keep_md: yes
    toc: yes
---

## Executive summary

The goal of this project is to predict how people performed the exercise based on data about personal movement that were collected from accelerometers on the arm, belt, dumbbell, and forearm when lifting the barbell. The dataset was collected from 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. This is the "classe" variable in the training dataset. We also have the test dataset (without "classe" variable) to predict the correctness of exercise performance in 20 different test cases using our best model.

The report will be focusing on:

- cleaning and preprocessing data
- building several machine learning models  
- performing for each model
    * cross validation  
    * model evaluation
- choosing the best model
- making prediction

**Results**

We built four models: two for each algorithm random forest and decision tree, using the packages `caret`, `randomForest` and `rpart`. Based on the results of the accuracy assessment, the first model of those considered was chosen to complete the final test.

## Data cleaning

[The training dataset](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

[The test dataset](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

More information is available from the website here: [Human Activity Recognition](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har) (see the section on the **Weight Lifting Exercise Dataset**).

```{r dataloading}
pmltraining <- read.csv("pml-training.csv", header = TRUE)
pmltesting  <- read.csv("pml-testing.csv", header = TRUE)

table0 <- data.frame(
    "Dataset"=c("pmltraining","pmltesting"),
    "Observations"=c(dim(pmltraining)[1],dim(pmltesting)[1]),
    "Variables"=c(dim(pmltraining)[2],dim(pmltesting)[2]))
table0
```

Data distribution on "classe" in the training dataset.

```{r classetable}

table(pmltraining$classe)
```

First of all, it is necessary to determine missing values and their approximate ratio in available data.

```{r table1, message=FALSE}
table1 <- data.frame(
    "Dataset"=c("pmltraining","pmltesting"),
    "Total number of NA"=c(sum(is.na(pmltraining)),sum(is.na(pmltesting))),
    "Percentage Ratio"=c(
        paste(round(sum(is.na(pmltraining))/(length(pmltraining)*nrow(pmltraining)), 2)*100, "%"),
        paste(round(sum(is.na(pmltesting))/(length(pmltesting)*nrow(pmltesting)), 2)*100, "%")))
table1
```

Thus, the percentage of NA's in the training and test datasets is 41% and 62%, respectively. In our case, imputing missing values can be tricky and might have a negative effect on accuracy when training our model. Let's remove them. Get rid of those variables that have NA in their columns.

```{r nona, message=FALSE}
training <- pmltraining[,colSums(is.na(pmltraining)) == 0]
testing <- pmltesting[,colSums(is.na(pmltesting)) == 0]
table3 <- data.frame(
    "Dataset"=c("training","testing"),
    "Observations"=c(dim(training)[1],dim(testing)[1]),
    "Variables"=c(dim(training)[2],dim(testing)[2]),
    "Total number of NA"=c(sum(is.na(training)),sum(is.na(testing))))
table3
```

Next, let's remove from the training dataset all the variables that are definitely not in the test dataset (except "classe").

```{r nona2}
trainingNEW <- training[,(colnames(training) %in% colnames(testing)) | (colnames(training) == "classe")]
str(trainingNEW, list.len = 10)
```

It also makes sense to remove variables that do not contribute to the accelerometer measurements, namely the first seven columns.

```{r oneseven}
trainingNEW <- trainingNEW[,-(1:7)]
testing <- testing[,-(1:7)]
table4 <- data.frame(
    "Dataset"=c("trainingNEW","testing"),
    "Observations"=c(dim(trainingNEW)[1],dim(testing)[1]),
    "Variables"=c(dim(trainingNEW)[2],dim(testing)[2]))
table4
```

Finally, we want to identify variables with near zero variance, if they are in our datasets.

```{r nzv, message=FALSE}
library(caret)
table5 <- data.frame(
    "Dataset"=c("trainingNEW","testing"),
    "Number of variables with near zero variance"=c(
        sum(nearZeroVar(trainingNEW, saveMetrics= TRUE)$nzv),
        sum(nearZeroVar(testing, saveMetrics= TRUE)$nzv)))
table5
```

When constructing models using some methods, we must be sure that our predictor is a factor.

```{r fact}
trainingNEW$classe <- as.factor(trainingNEW$classe)
class(trainingNEW$classe)
```

## Data splitting

For further cross validation let's create data partitions using given "classe" variable.

```{r splitdata}
set.seed(10001)
intrain <- createDataPartition(trainingNEW$classe, p=.8, list=FALSE)
training <- trainingNEW[intrain,]
validation <- trainingNEW[-intrain,] 
table6 <- data.frame("Dataset"=c("Original dataset","Training dataset","Validating dataset"),
    "Observations"=c(dim(trainingNEW)[1],dim(training)[1],dim(validation)[1]),
    "Variables"=c(dim(trainingNEW)[2],dim(training)[2],dim(validation)[2]))
table6
```

## Model 1: Random Forest (`randomForest`)

"The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree." ([from here](https://towardsdatascience.com/understanding-random-forest-58381e0602d2))

To train algorithm on training data in this first model we use the `randomForest` package that implements Breiman’s random forest algorithm for classification and regression.  

```{r randomForest1, message=FALSE}
library(randomForest)
set.seed(10002)
fitRF1 <- randomForest(classe ~ ., data = training)
fitRF1
```

Figure below shows a measure of how important some variables are for estimating the value of the target variable across all of the trees that make up the forest. A higher Mean Decrease in Gini indicates higher variable importance. Gini Importance or Mean Decrease in Impurity (MDI) calculates each feature importance as the sum over the number of splits (across all tress) that include the feature, proportionally to the number of samples it splits.

```{r fig1}
varImpPlot(fitRF1, n.var=10, main = "Top 10 variables of importance")
```

Predicting on validation dataset using our model.

```{r predict1}
predictRF1 <- predict(fitRF1, validation)
```

Evaluating the accuracy of our model on new (validating) data contains also the expected out of sample error. It is the error resulted from applying our prediction algorithm to a new (validating) data set. The out of sample error most important as it better evaluates how the model should perform.

```{r confusionMatrix1}
result11 <- confusionMatrix(predictRF1, validation$classe)
table7 <- data.frame("Accuracy"=as.numeric(result11$overall['Accuracy']),
                     "Out of sample error"=(1 - as.numeric(result11$overall['Accuracy'])))
table7
```

Confusion matrix.

```{r confusionMatrix2}
table(observed = validation$classe, predicted = predictRF1)
```

## Model 2: Random Forest (`caret`)

For the next model we use `caret` package. We can train algorithms on training data and apply to validation set using `train()` and `predict()`. To perform cross validation we set the resampling option `trControl`. The resampling method is "cv" and the number of folds (or number of resampling iterations) equal 5.

```{r randomForest2, message=FALSE}
set.seed(10003)
fitRF2 <- train(classe ~ ., data = training, method = "rf", 
                trControl = trainControl(method = "cv", 5), ntree = 100)
fitRF2
fitRF2$finalModel
varImpPlot(fitRF2$finalModel, n.var=10, main = "Top 10 variables of importance")
```

Note that the list of top 10 important variables of the previous model and this one, as well as the measure of their importance are very close.

```{r fig2, out.width = "80%"}
par(mfrow=c(1,2))
varImpPlot(fitRF1, n.var=10, main = "RF1")
varImpPlot(fitRF2$finalModel, n.var=10, main = "RF2")
par(mfrow=c(1,1))
```

Predicting on validating dataset using our model.

```{r predict2}
predictRF2 <- predict(fitRF2, validation)
```

Evaluating the accuracy of this model on new (validating) data.

```{r confusionMatrix3}
result12 <- confusionMatrix(predictRF2, validation$classe)
table8 <- data.frame("Accuracy"=as.numeric(result12$overall['Accuracy']),
                     "Out of sample error"=(1 - as.numeric(result12$overall['Accuracy'])))
table8
```

## Model 3: Decision Tree (`rpart`)

"The decision tree algorithm tries to solve the problem, by using tree representation. Each internal node of the tree corresponds to an attribute, and each leaf node corresponds to a class label.  

Decision Tree Algorithm Pseudocode:  

- Place the best attribute of the dataset at the root of the tree.  
- Split the training set into subsets. Subsets should be made in such a way that each subset contains data with the same value for an attribute.  
- Repeat step 1 and step 2 on each subset until you find leaf nodes in all the branches of the tree." ([from here](https://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/))

To build this model, we use the `rpart` package that do recursive partitioning for classification, regression and survival trees. We again train algorithm on training data and apply to validation set.

```{r rpart1, out.width = "110%"}
set.seed(10004)
library(rpart); library(rpart.plot) 
fitDT1 <- rpart(classe ~., data=training, method="class")
prp(fitDT1)
predictDT1 <- predict(fitDT1, validation, type = "class") 
result21 <- confusionMatrix(predictDT1, validation$classe)
table9 <- data.frame("Accuracy"=as.numeric(result21$overall['Accuracy']),
                     "Out of sample error"=(1 - as.numeric(result21$overall['Accuracy'])))
table9
```

## Model 4: Decision Tree (`caret`)

The last model in this report uses package `caret` to train the decision tree algorithm, and as in previous models, applies the result to a validating dataset to make prediction, followed by an assessment of accuracy.

```{r rpart2, out.width = "110%"}
set.seed(10005)
fitDT2 <- train(classe ~ ., data = training, method = "rpart", trControl = trainControl(method = "cv", 5))
fitDT2
rpart.plot(fitDT2$finalModel, type = 3, clip.right.labs = FALSE, under = TRUE)
predictDT2 <- predict(fitDT2, validation)
result22 <- confusionMatrix(predictDT2, validation$classe)
table10 <- data.frame("Accuracy"=as.numeric(result22$overall['Accuracy']),
                     "Out of sample error"=(1 - as.numeric(result22$overall['Accuracy'])))
table10
```

## Сhoosing the best model

Let's look at the accuracy comparison table of all our models.

```{r fin}
table11 <- data.frame(
    "Model"=c("RF1", "RF2", "DT1", "DT2"),
    "Accuracy"=c(paste(round(as.numeric(result11$overall['Accuracy']),4)*100, "%"),
                 paste(round(as.numeric(result12$overall['Accuracy']),4)*100, "%"),
                 paste(round(as.numeric(result21$overall['Accuracy']),4)*100, "%"),
                 paste(round(as.numeric(result22$overall['Accuracy']),4)*100, "%")),
    "Out of sample error"=c(
        paste(round((1 - as.numeric(result11$overall['Accuracy'])),4)*100, "%"),
        paste(round((1 - as.numeric(result12$overall['Accuracy'])),4)*100, "%"),
        paste(round((1 - as.numeric(result21$overall['Accuracy'])),4)*100, "%"),
        paste(round((1 - as.numeric(result22$overall['Accuracy'])),4)*100, "%")))
table11
```

We know that it is often better to abandon the greatest accuracy for more robustness when predicting on new data. The reason is over-fitting: model can be too adapted (optimized) for the initial dataset, and predictor won’t perform as well on new sample. But in this case, we essentially choose one of two algorithms, namely random forest and classification tree, and in this case, the random forest algorithm gave the best result.

## Predicting on test dataset

Now we can fulfill the prediction of outcome levels on the original testing dataset using the first built model with random forest algorithm.

```{r pred}
predictRES <- predict(fitRF1, testing, type="class")
predictRES
```
